{
    "Assets": {
      "Model": {
        "Model Weights": {
          "Threats": [
            {
              "Name": "Model Inversion",
              "Definition": "Attacker reconstruct the model's parametrs or architecture by probing the models output",
              "Mitigation": [
                "Reactive: input sanitization to remove any adversarial pertubations",
                "Proactive: train model on adversarial examples to improve robustness",
                "Implement strong access controls to limit access to sensitive data used in the model"
              ]
              
            },
            {
              "Name": "Insecure Access",
              "Definition": "Poorly secured storage or access controls allow unauthroized retrieval of model weights",
              "Mitigation": ["Encrypt data; enforce strict IAM; audit logs."]
            },
            {
              "Name": "Insecure Pipeline",
              "Definition": "Vulnerabilities in the build or deployment pipeline could allow for tampering or leakage of weights",
              "Mitigation": [
                "Use code signing to verify build",
                "implement security checks in the CI/CD build"
                ]
            }
          ]
        },
        "Training Data": {
          "Threats": [
            {
              "Name": "Split-View Data Poisoning",
              "Definition": "Different version of data is presented to different systems or users - when a scraper is detected, serve poisoned data",
              "Mitigation": [
                "Use authenticated data sources; validate data provenance.",  
                "Implemet hashing for indexed context - ensure that model creators get the same data as when the dataset maintainers indexed and hashed them",
                "Rotate scraper signatures"
              ]
            },
            {
              "Name": "Frontrunning Data Poisoning",
              "Definition": "providing malicious or biased data to the training set before an authorative source is provided",
              "Mitigation": [
                "Use authenticated data sources; validate data provenance.",
                "Introduce randomization in the scheduling of snapshots."
              ]
              

            },
            {
              "Name": "Using Inadequate or Unauthorized Data",
              "Definition": "training on low-quality, biased, or private datasets without proper vetting or permissions",
              "Mitigation": ["Vet datasets and verify licenses."]
            },
            {
              "Name": "Model Source Tampering",
              "Definition": "malicious actors modify the data sources or inject fake content into scraped data sets",
              "Mitigation": [
                "Use authenticated data sources; validate data provenance.",
                "Poison detection to identify anomolies to identify outliers"
              ]
            },
            {
              "Name": "Model Weight Tampering",
              "Definition": "Altering pre-trained model file or artifacts with hidden behaviors or vulnerabilities",
              "Mitigation": [
                "Vet datasets and verify licenses",
                "Implement strong access controls to limit access to model weights"
              ]
            }
          ]
        },
        "Embedding Vectors": {
          "Threats": [
            {
              "Name": "Vector Embedding Weakness",
              "Definition": "Embeddings may leak sensitive data or expose relationships that can be exploited",
              "Mitigation": ["Apply noise; limit embedding exposure."]
            },
            {
              "Name": "Inversion attack on embeddings",
              "Definition": "Attackers reverse-engineer input content or private information from vector outputs",
              "Mitigation": ["Encrypt vector embeddings"]
            }
          ]
        }
      },
      "Infrastructure": {
        "API Keys and Tokens": {
          "Threats": [
            {
              "Name": "Sensitive Data Exposure",
              "Definition": "Leaked or mismanaged credentials can allow unauthorized access to internal systems",
              "Mitigation": [
                "Use secrets managers",
                "enforce least privilege",
                "scrub sensitive output"
              ]
            }
          ]
        },
        "Compute Resources": {
          "Threats": [
            {
              "Name": "Insecure Supply Chain",
              "Definition": "vulnerable dependencies can introduce backdoors",
              "Mitigation": ["Use dependency scanners; restrict builds to known-good sources."]
            },
            {
              "Name": "Insecure Access",
              "Definition": "lead to unauthorized user access to critical systems",
              "Mitigation": [
                "Encrypt data",
                "enforce strict IAM",
                "enable and review audit logs"
              ]
            }
          ]
        },
        "Deployment Pipelines": {
          "Threats": [
            {
              "Name": "Insecure Supply Chain",
              "Definition": "vulnerable dependencies can introduce backdoors",
              "Mitigation": [
                "Use dependency scanners",
                "restrict builds to known-good sources."
              ]
            },
            {
              "Name": "Insecure Access",
              "Definition": "Lead to unauthorized user access to critical systems",
              "Mitigation": [
                "Encrypt data",
                "enforce strict IAM",
                "enable and review audit logs"
              ]
            }
          ]
        },
        "Inference Platform": {
          "Threats": [
            {
                "Name": "side-channel timing attacks",
                "Definition": "analysis of the time to make an inference can reveal information about the model's layers or parameters",
                "Mitigation": ["Utilize random lenth noise in the token responses to obscure the length of the token"]
            },
            {
                "Name": "denial of service",
                "Definition": "overload the inference service with high-volume or malicious inputs, leading to degraded availability",
                "Mitigation": [
                  "Rate Limiting and throttling",
                  "Logging and monitoring for malicious use"]
            }
          ]
        },
        "LoRA Adapter": {
            "Threats": [
                {
                    "Name": "LoRA Injection",
                    "Definition": "Malicious LoRA weights to manipulate model behavior post-deployment",
                    "Mitigation": ["Change management for introducing LoRA adapters"]
                }
            ]
        }
      },
      "Application": {
        "End-User Applications": {
          "Threats": [
            {
              "Name": "Prompt Injection via Jailbreak",
              "Definition": "carefully creafted prompts bypass constraints to trigger unauthorized behaviors",
              "Mitigation": [
                "Sanitize inputs",
                "restrict prompt scope",
                "Use retrieval augmentation."
              ]
            },
            {
              "Name": "Indirect Prompt Injection",
              "Definition": "Model reads adversarial content that manipulates outputs without direct interaction",
              "Mitigation": [
                "Validate content sources", 
                "avoid echoing untrusted inputs."
              ]
            }
          ]
        },
        "End-User APIs": {
          "Threats": [
            {
              "Name": "Insecure Access",
              "Definition": "poor authentication, rate-limiting, input validation allow for misuse or exploitation",
              "Mitigation": [
                "Encrypt data",
                "enforce strict IAM",
                "enable and review audit logs"
              ]
            },
            {
              "Name": "Sensitive Data Exposure",
              "Definition": "poor input/output handling leads to sensitive data exposure",
              "Mitigation": [
                "Use secrets managers",
                "enforce least privilege",
                "scrub sensitive output."
              ]
            },
            {
              "Name": "Prompt Injection via Jailbreak",
              "Definition": "carefully creafted prompts bypass constraints to trigger unauthorized behaviors",
              "Mitigation": [
                "Sanitize inputs",
                "restrict prompt scope", 
                "use retrieval augmentation."
            ]
            },
            {
              "Name": "Indirect Prompt Injection",
              "Definition": "Model reads adversarial content that manipulates outputs without direct interaction",
              "Mitigation": "Validate content sources; avoid echoing untrusted inputs."
            }
          ]
        },
        "User Feedback Data": {
          "Threats": [
            {
              "Name": "Training on Personal Data",
              "Definition": "Feedback loops may include personal information, leading to privacy violations if reused in training",
              "Mitigation": [
                "Differential privacy - obscure user provided data points",
                "Federadted Learning - local model sends updates to global mode - raw data is not set only gradients and weights",
                "Utilize homomorphic encryption to allow computation on encrypted data w/o decryption",
                "Utilize sensitive personsal data filters to enable tokenzation or encryption"
              ]
            },
            {
              "Name": "Feedback poisioning",
              "Definition": "malicious feedback is submitted to influence model updates, behavior, or fine-tuning",
              "Mitigation": ["Implement access controls to ensure feedback comes from authenticated, trusted users"]
            }
          ]
        }
      }
    }
  }